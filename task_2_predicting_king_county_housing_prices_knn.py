# -*- coding: utf-8 -*-
"""Task_2_Predicting_King_County_Housing_Prices_knn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fsNm8yCQ2rIlW19dYYHbpWVycW-qX-xU

# K Nearest Neighbors: Predicting King County Housing Prices

Dataset
The dataset is available at "data/kc_house_data.csv" in the respective challenge's repo.
Original Source: https://www.kaggle.com/shivachandel/kc-house-data 



### How would you predict the price of a house that is about to go on sale?



Online property companies offer valuations of houses using machine learning techniques. The aim of this report is to predict the house sales in King County, Washington State, USA using Multiple Linear Regression (MLR). The dataset consisted of historic data of houses sold between May 2014 to May 2015.
We will predict the sales of houses in King County with an accuracy of at least 75-80% and understand which factors are responsible for higher property value - $650K and above.”

The dataset consists of house prices from King County an area in the US State of Washington, this data also covers Seattle. 



The data set contains 21613 observations (home sales in 2014-15) with 19 features plus house price. Descriptions and names of the columns (features) are given below.

## Features
1.	date: Date house was sold
2.	price: Price of the sold house
3.	bedrooms: Number of Bedrooms
4.	bathrooms: Number of bathrooms
5.	sqft_living: Square footage of the living space
6.	sqrt_lot: Square footage of the lot
7.	floors: Total floors in the house
8.	waterfront: Whether the house is on a waterfront(1: yes, 0: no)
9.	view: special view?
10.	condition: Condition of the house
11.	grade: unknown
12.	sqft_above: Square footage of house apart from basement
13.	sqft_basement: Square footage of the basement
14.	yr_built: Built year
15.	yr_renovated: Year when the house was renovated
16.	zipcode: zipcode of the house
17.	lat: Latitude coordinate
18.	long Longitude coordinate
19.	sqft_living15: Living room area in 2015(implies some renovations)
20.	sqrt_lot15: Lot area in 2015(implies some renovations)

### X = house_data[features]
### y = house_data['price'].values

## Similar houses should be similar in price

* Square footage
* Number of floors
* Location


## Distance as a measure of similarity

How 'far away' are houses from each other given all of their features?

## What is K-Nearest Neighbors?

**_K-Nearest Neighbors_** (or KNN, for short) is a supervised learning algorithm that can be used for both **_Classification_** and **_Regression_** tasks. KNN is a distance-based classifier, meaning that it implicitly assumes that the smaller the distance between 2 points, the more similar they are. In KNN, each column acts as a dimension. In a dataset with two columns, we can easily visualize this by treating values for one column as X coordinates and and the other as Y coordinates. Since this is a **_Supervised Learning Algorithm_**, we must also have the labels for each point in our dataset, or else we can't use this algorithm for prediction.

## Fitting the Model

KNN is unique compared to other algorithms in that it does almost nothing during the "fit" step, and all the work during the "predict" step. During the 'fit' step, KNN just stores all the training data and corresponding values. No distances are calculated at this point. 

## Making Predictions with K

All the magic happens during the 'predict' step. During this step, KNN takes a point that we want a class prediction for, and calculates the distances between that point and every single point in the training set. It then finds the `K` closest points, or **_Neighbors_**, and examines the values of each. You can think of each of the K-closest points getting a 'vote' about the predicted value. Often times the mean of all the values is taken to make a prediction about the new point.

In the following animation, K=3.

<img src='https://github.com/Bmcgarry194/knn_workshop/blob/master/knn.gif?raw=1'>

## Distance Metrics

As we explored in a previous lesson, there are different **_distance metrics_** when using KNN. For KNN, we can use **_Manhattan_**, **_Euclidean_**, or **_Minkowski Distance_**--from an algorithmic standpoint, it doesn't matter which! However, it should be noted that from a practical standpoint, these can affect our results and our overall model performance.

Tasks
1.	Compare KNN Classifier and Logistic Regression
2.	Creating our own implementation of KNN regressor by using generate_moons_df i.e., random dataset
3.	Housing data predictions
4.	Limit our predictions to the middle 80% of our dataset
5.	Apply data scaling
6.	Predict data using your own knn
7.	Predict data using sklearn’s knn
8.	Choosing the optimal number of neighbors: Model behavior with increasing k for classification problem
9.	Finding optimal k for King County Dataset


Download and load the data (csv file contains ';' as delimiter)
"""

pip install neat-python

import matplotlib.pyplot as plt
import seaborn as sns
from scipy.spatial.distance import euclidean as euc
import numpy as np
import pandas as pd
import neat
from sklearn.datasets import make_classification
#import neat.visualize as visualize
from Mask_RCNN.mrcnn import visualize
from visualize import generate_moons_df, preprocess, plot_boundaries

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
np.random.seed(0)

"""### Compare KNN Classifier and Logistic Regression"""

X_train_scaled, X_test_scaled, y_train, y_test = preprocess(generate_moons_df(n_samples= 20, noise=0.1))

# fit knn model
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train_scaled.drop('target', axis=1), y_train)

# fit logistic
logistic = LogisticRegression(solver='liblinear')
logistic.fit(X_train_scaled.drop('target', axis=1), y_train);

fig, axes = plt.subplots(ncols=2, figsize=(14, 6), sharey=True)

axes[0].set_title(f'KNeighborsClassifier with k={knn.n_neighbors}')
plot_boundaries(knn, ?,?, ax=axes[0], show_test=True, plot_probas=False)

axes[1].set_title('Logistic Regression')
plot_boundaries(logistic, ?,?, ax=axes[1], plot_probas=False, show_test=True)

"""As we can see here logistic regression without any feature engineering can only give us a linear boundary but KNN can begin to follow the non-linear relationships in our data already.

The dots outlined in black represent data points from our test set. Looking at these points can we calculate our accuracy? False positives, False Negatives?

## Fitting and Transforming

Sklearn is one of the most popular ML libraries for python which gives us access to a wealth of different algorthims. All of these algorthims follow the same API

```python
model = model_object()

model.fit()

model.predict()```

## Creating our own implementation of KNN regressor
"""

class KNN():
    pass

"""## Housing data predictions"""

house_data = pd.read_csv('?')

"""## Limit our predictions to the middle 80% of our dataset

It is easier to make predictions where the data is most dense but doing this means that any predictions made outside of the range of values we are training on will be highly suspect
"""

bottom_10 = np.percentile(house_data['price'], 10)
top_10 = np.percentile(house_data['price'], 90)

house_data = house_data[(house_data['price'] > ?) & (house_data['price'] < ?]

fig, ax = plt.subplots()
ax.hist(house_data['price'], bins=50);

features = ['sqft_living', 'lat', 'long']

X = ?
y = ?

X_train, X_test, y_train, y_test = train_test_split(?, ?)

"""## Why do we need to scale our data?"""

scaler = StandardScaler()

scaler.fit(?)

X_train_scaled = scaler.transform(?)

X_test_scaled = scaler.transform(?)

my_knn = KNN()
my_knn.fit(?, ?)

X_train_scaled.shape

"""## Why is this so slow?"""

#This will run for a long time
preds = my_knn.predict(?, k=3)

X_test_scaled.shape

"""## Lets use Sklearn's KNN implementation"""

from sklearn.neighbors import KNeighborsRegressor

nn = KNeighborsRegressor(n_neighbors=5, n_jobs=-1)

nn.fit(?, ?)

sk_preds = nn.predict(?)

rmse = np.sqrt(mean_squared_error(?,?))

print(f'Root Mean Squared Error: {rmse:.2f}')

"""## Choosing the optimal number of neighbors: Model behavior with increasing k"""

# These are visualizations of a classification problem

fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(14, 10), sharey=True, sharex=True)

X_train_scaled_fake, X_test_scaled_fake, y_train_fake, y_test_fake = preprocess(generate_moons_df(n_samples=100, noise=0.1))

ks = [1, 2, 5, 10 , 15, 30]

for k, ax in zip(ks, axes.flatten()):
    
    knn = KNeighborsClassifier(?)
    knn.fit(X_train_scaled_fake.drop('target', axis=1), ?)
    
    train_preds = knn.score(X_train_scaled_fake.drop('target', axis=1), ?)
    test_preds = knn.score(X_test_scaled_fake.drop('target', axis=1), ?)
    
    ax.set_title(f'k={knn.n_neighbors} \n train acc {train_preds:.2f} \n test acc: {test_preds:.2f}')
    plot_boundaries(knn, ?,?, ax=ax)
    
fig.tight_layout()

"""What can we say about the bias and variance of these models?

## Finding optimal k for King County Dataset
"""

ks = range(1, 30)

test_errors = np.zeros(len(list(ks)))

for i, k in enumerate(ks):
    
    nn = KNeighborsRegressor(n_neighbors=?, n_jobs=-1)

    nn.fit(?,?)
    test_preds = nn.predict(?)
    
    test_errors[i] = np.sqrt(mean_squared_error(?,?))

fig, ax = plt.subplots(figsize=(10, 5))

ax.plot(list(ks), test_errors)
ax.axvline(list(ks)[np.argmin(test_errors)], linestyle='--', color='black');

optimal_k = list(ks)[np.argmin(test_errors)]

optimal_error = np.min(test_errors)

print(f'Optimal number of Neighbors: {optimal_k} Root Mean Squared Error: {optimal_error:.2f}')